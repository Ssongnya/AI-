# 토큰화(Tokenization)와 임베딩(Embedding)

## 1. 토큰화(Tokenization)

### (1) 정의
- 문장은 모델에 바로 대입할 수 없다 따라서 **토큰화**가 필요하다.
- 문장을 **토큰(token)** 단위로 나누는 과정  
- 즉, 긴 텍스트를 단어, 형태소, 서브워드(subword) 등의 작은 단위로 분리하여  
  **모델이 이해할 수 있는 입력 형태로 변환**하는 단계

### (2) 목적
- 자연어를 컴퓨터가 처리 가능한 형태(숫자 또는 벡터)로 변환하기 위한 전처리 과정
- 문장 구조를 분석하고, 단어 간의 관계를 학습할 수 있도록 도와줌

### (3) 예시
| 입력 문장 | 토큰화 결과 |
|------------|--------------|
| "나는 학교에 간다" | ["나", "는", "학교", "에", "간다"] |
| "ChatGPT is powerful" | ["ChatGPT", "is", "powerful"] |


## 2. 임베딩(Embedding)

### (1) 정의
- **토큰화된 단어에 의미를 부여**하는 과정  
- 각 단어를 고정된 크기의 **벡터(Vector)** 로 표현하여, 단어 간의 의미적 관계를 수치로 학습할 수 있게 함

### (2) 특징
- 벡터 공간에서 의미가 비슷한 단어들은 **가까운 위치**에 위치함  
- 예:  
  `king - man + woman ≈ queen`  
  → 의미 공간에서 유사한 관계를 벡터 연산으로 표현 가능

### (3) 임베딩 표현
- 각 단어는 `n차원 공간`의 **수치 벡터(vector)** 로 표현됨  
- 이 벡터는 학습 과정에서 모델이 자동으로 업데이트함

| 단어 | 벡터 표현(예시) |
|------|----------------|
| king | [0.25, -0.13, 0.78, …] |
| queen | [0.24, -0.10, 0.75, …] |


## 3. 토큰화와 임베딩의 관계

| 단계 | 과정 | 설명 |
|------|------|------|
| ① | **토큰화(Tokenization)** | 문장을 단어/형태소 단위로 분리 |
| ② | **임베딩(Embedding)** | 각 토큰을 수치 벡터로 변환해 의미를 표현 |
| ③ | **모델 입력(Input)** | 벡터화된 데이터를 신경망에 전달 |


## 4. 임베딩 시각화 (t-SNE 예시)
- 임베딩된 단어 벡터를 2차원 공간에 시각화하면,  
  비슷한 의미의 단어들이 **자연스럽게 군집(Cluster)** 을 형성함.
- 예:  
  장르별 소설 임베딩을 시각화했을 때,  
  "Fantasy", "Science Fiction", "Children's Novel" 등이 **서로 가까운 영역에 분포**.


## 5. 핵심 요약

| 구분 | 토큰화 | 임베딩 |
|------|--------|--------|
| 역할 | 문장을 단어 단위로 분리 | 단어를 의미 벡터로 변환 |
| 입력 형태 | 텍스트 문자열 | 토큰 ID 또는 단어 벡터 |
| 출력 형태 | 토큰 리스트 | 고차원 벡터 공간 내 좌표 |
| 주요 목적 | 텍스트 구조화 | 의미 표현 및 관계 학습 |
| 대표 예시 | WordPiece, BPE | Word2Vec, GloVe, BERT Embedding |


✅ **정리**
- 토큰화는 **언어를 나누는 과정**,  
- 임베딩은 **그 나눠진 언어에 의미를 부여하는 과정**.  
- 두 단계는 NLP의 전처리와 표현 학습의 핵심이며,  
  모델이 자연어를 이해하고 처리할 수 있도록 돕는 필수 기반이다.
