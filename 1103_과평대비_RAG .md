# RAG (Retrieval-Augmented Generation) 기본 개념 정리

## 1. RAG의 개념

### (1) 정의
- **RAG (Retrieval-Augmented Generation)**  
  → LLM이 **신뢰성 있는 외부 지식**을 검색(Retrieval)하여 답변 생성(Generation)에 반영하는 기술.  
- 모델이 이미 학습한 파라미터만 사용하는 것이 아니라,  
  **최신 정보**를 참고함으로써 **사실성(Factuality)** 과 **정확성**을 높인다.

### (2) 목적
- LLM이 학습 데이터에 없는 정보를 외부에서 가져와 활용
- **“환각(Hallucination)” 현상 감소** — 즉, 근거 없는 답변을 줄임
- 모델을 다시 학습시키지 않고도 **지식 업데이트 가능**


## 2. RAG의 전체 구조

RAG는 크게 **두 단계**로 구성된다.

### (1) Retrieval 단계
- **질의(Query)** 를 기반으로 관련 문서를 **벡터 검색(Vector Search)** 을 통해 찾음  
- 외부 데이터 저장소(예: PDF, 웹 문서, 데이터베이스 등)에 임베딩 형태로 저장
- 검색된 문서들은 **LLM의 입력으로 전달될 컨텍스트(Context)** 역할 수행

### (2) Generation 단계
- 검색된 문서(Context)와 원래 사용자의 질문을 함께 입력하여  
  LLM이 **근거 있는 응답(Answer)** 을 생성
- LLM은 “문서를 참고한 답변”을 생성하므로 정확도와 신뢰도가 향상됨


## 3. LangChain과의 관계

### (1) LangChain의 역할
- RAG를 **쉽게 구현하기 위한 프레임워크**  
- LLM + 벡터DB + 임베딩 + 프롬프트 구성요소를 체계적으로 연결해줌  
- 주요 모듈:
  - **Document Loader**: 문서 불러오기
  - **Text Splitter**: 문서 분할
  - **Embedding Model**: 문서를 벡터로 변환
  - **Vector Store**: 벡터 형태로 저장 및 검색
  - **Retriever**: 쿼리에 맞는 문서 검색
  - **LLM Chain**: 검색 결과를 LLM에 전달해 답변 생성


## 4. RAG의 구성 요소별 핵심 역할

| 구성 요소 | 설명 | 예시 |
|-------------|------|------|
| **문서(Document)** | 검색 대상이 되는 외부 데이터 | PDF, 웹 텍스트 등 |
| **임베딩(Embedding)** | 문장을 벡터로 변환하여 유사도 계산 | SentenceTransformer 등 |
| **벡터DB(Vector Store)** | 임베딩된 문서 저장 및 검색 수행 | FAISS, Chroma 등 |
| **Retriever** | 사용자의 질문과 유사한 문서 검색 | similarity_search() |
| **LLM (Generator)** | 검색된 문서 기반으로 답변 생성 | GPT, Claude 등 |


## 5. RAG의 작동 흐름 요약

1️⃣ **문서 로드** → 2️⃣ **문서 분할** → 3️⃣ **임베딩 생성**  
→ 4️⃣ **벡터 저장소에 저장** → 5️⃣ **질문 입력 시 유사 문서 검색(Retrieval)**  
→ 6️⃣ **검색된 문서 + 질문을 LLM에 입력** → 7️⃣ **답변 생성(Generation)**


## 6. RAG의 장점과 한계

### ✅ 장점
- **최신 정보 반영 가능** (재학습 불필요)
- **환각(Hallucination) 감소**
- **특정 도메인 지식 추가 용이**
- **효율적인 메모리 사용**

### ⚠️ 한계
- 검색 품질이 낮으면 답변 품질도 떨어짐
- 문서 임베딩 및 검색 속도 이슈
- 완전한 “사실 검증 시스템”은 아님


## 7. 시험 대비 핵심 포인트 정리

| 핵심 용어 | 의미 / 역할 |
|------------|--------------|
| **RAG** | 외부 지식 검색을 결합한 LLM 답변 생성 구조 |
| **Retrieval** | 쿼리와 유사한 문서 검색 단계 |
| **Generation** | 검색된 문서 기반으로 답변 생성 |
| **LangChain** | RAG 파이프라인 구성 및 연결을 자동화하는 프레임워크 |
| **Embedding & VectorDB** | 문서를 벡터로 변환·저장하여 검색 가능하게 하는 핵심 구성요소 |
| **목적** | 최신 지식 활용 + 환각 현상 감소 |


✅ **한 줄 요약**
> RAG는 LLM이 외부 지식을 실시간으로 검색해 답변 생성에 반영하는 기술로,  
> LangChain을 활용하면 문서 임베딩부터 검색·생성까지 전체 파이프라인을 쉽게 구현할 수 있다.
